{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15d6ea56",
   "metadata": {},
   "source": [
    "# ðŸ§  Enterprise IT Observability Agent Capstone\n",
    "\n",
    "Welcome! This notebook is a production-ready adaptation of the **Google/Kaggle 5-Day Agents Intensive** templates. It aligns with the official structure while delivering the **Enterprise IT Observability** capstone project requirements:\n",
    "\n",
    "- âœ… Adopt the [Agent Development Kit (ADK)](https://google.github.io/adk-docs) foundations taught during the course.\n",
    "- âœ… Demonstrate a **Level 3 collaborative multi-agent system** with a pathway toward **Level 4 self-evolving behavior**.\n",
    "- âœ… Simulate real-world infrastructure telemetry (logs, metrics, incident emails).\n",
    "- âœ… Provide repeatable experiment blocks so you can continue iterating toward deployment.\n",
    "\n",
    "> ðŸ“… **Deadline reminder:** Submission due **Dec 1, 2025** (13 days remaining). Stay focused and track progress as you run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e766d287",
   "metadata": {},
   "source": [
    "## ðŸ“š Notebook Roadmap\n",
    "\n",
    "1. **Set up the environment** â€“ install dependencies and configure Google Cloud Vertex AI.\n",
    "2. **Simulate observability data** â€“ generate realistic logs, metrics, and incident emails.\n",
    "3. **Define ADK tools** â€“ wrap data access and analysis helpers as agent tools.\n",
    "4. **Compose the agent team** â€“ supervisor plus specialist agents for logs, metrics, and operations.\n",
    "5. **Run an observability scenario** â€“ orchestrated analysis of a slowdown incident.\n",
    "6. **Launch an interactive chat UI** â€“ prototype a management-facing dashboard.\n",
    "7. **Capture next steps** â€“ track tasks to reach Level 4 self-evolving capabilities.\n",
    "\n",
    "Each section mirrors the pedagogy of the official course notebooks so you can slot this work directly into your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "413460da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T07:43:24.260655Z",
     "iopub.status.busy": "2025-11-23T07:43:24.260268Z",
     "iopub.status.idle": "2025-11-23T07:43:24.265948Z",
     "shell.execute_reply": "2025-11-23T07:43:24.265060Z",
     "shell.execute_reply.started": "2025-11-23T07:43:24.260625Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# @title ðŸ› ï¸ Install core dependencies\n",
    "# This mirrors the setup pattern used in the Google/Kaggle course notebooks.\n",
    "# %pip install --quiet google-adk google-cloud-aiplatform pandas gradio #uncomment to install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f33f9438",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T07:43:24.268364Z",
     "iopub.status.busy": "2025-11-23T07:43:24.267726Z",
     "iopub.status.idle": "2025-11-23T07:43:24.552726Z",
     "shell.execute_reply": "2025-11-23T07:43:24.551419Z",
     "shell.execute_reply.started": "2025-11-23T07:43:24.268331Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Gemini API key loaded from .env file.\n"
     ]
    }
   ],
   "source": [
    "# @title ðŸ”‘ Load API key (Kaggle Secrets or local .env)\n",
    "import os\n",
    "\n",
    "# Try Kaggle Secrets first (when running on Kaggle)\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "    print(\"âœ… Gemini API key loaded from Kaggle Secrets.\")\n",
    "except ImportError:\n",
    "    # Fallback to .env file (when running locally)\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "        if GOOGLE_API_KEY:\n",
    "            print(\"âœ… Gemini API key loaded from .env file.\")\n",
    "        else:\n",
    "            raise ValueError(\"GOOGLE_API_KEY not found in environment variables\")\n",
    "    except ImportError:\n",
    "        # Manual environment variable check\n",
    "        GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "        if GOOGLE_API_KEY:\n",
    "            print(\"âœ… Gemini API key loaded from environment variables.\")\n",
    "        else:\n",
    "            print(\"âŒ Please set GOOGLE_API_KEY environment variable or install python-dotenv\")\n",
    "except Exception as e:\n",
    "    print(f\"ðŸ”‘ Authentication Error: {e}\")\n",
    "    print(\"Please ensure GOOGLE_API_KEY is available in Kaggle Secrets or environment variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1d17ff",
   "metadata": {},
   "source": [
    "## ðŸ” Authenticate & Configure Vertex AI\n",
    "\n",
    "Just like the official course materials, we authenticate first and then initialize Vertex AI. If you are running locally, make sure you have executed `gcloud auth application-default login` beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cc85a76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T07:43:24.553902Z",
     "iopub.status.busy": "2025-11-23T07:43:24.553650Z",
     "iopub.status.idle": "2025-11-23T07:43:47.761317Z",
     "shell.execute_reply": "2025-11-23T07:43:47.759991Z",
     "shell.execute_reply.started": "2025-11-23T07:43:24.553879Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.12) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using project: l3-multi-agent-system-4-IT | region: us-central1\n"
     ]
    }
   ],
   "source": [
    "# @title Authenticate and initialize Vertex AI\n",
    "import os\n",
    "import vertexai\n",
    "\n",
    "PROJECT_ID = os.getenv(\"GOOGLE_CLOUD_PROJECT\", \"l3-multi-agent-system-4-IT\")  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "MODEL_NAME = \"gemini-2.5-flash-lite\"\n",
    "\n",
    "print(f\"Using project: {PROJECT_ID} | region: {LOCATION}\")\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "776a1655",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T07:43:47.764036Z",
     "iopub.status.busy": "2025-11-23T07:43:47.763358Z",
     "iopub.status.idle": "2025-11-23T07:43:47.777360Z",
     "shell.execute_reply": "2025-11-23T07:43:47.775843Z",
     "shell.execute_reply.started": "2025-11-23T07:43:47.764004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# @title Generate synthetic logs, metrics, and incidents\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Literal\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "SEVERITIES = [\"CRITICAL\", \"ERROR\", \"WARN\", \"INFO\"]\n",
    "INCIDENT_TYPES = [\"Network\", \"Database\", \"Application\", \"Infrastructure\"]\n",
    "\n",
    "\n",
    "def generate_mock_logs(server_id: str, *, window_minutes: int = 240) -> str:\n",
    "    \"\"\"Create timestamped log entries with realistic error bursts.\"\"\"\n",
    "    now = datetime.utcnow()\n",
    "    entries = []\n",
    "    for minute in range(window_minutes // 5):\n",
    "        timestamp = now - timedelta(minutes=minute * 5)\n",
    "        level = random.choices(SEVERITIES, weights=[0.05, 0.15, 0.3, 0.5])[0]\n",
    "        if level in {\"CRITICAL\", \"ERROR\"}:\n",
    "            message = random.choice([\n",
    "                \"Latency spike detected on API Gateway\",\n",
    "                \"Database connection timeout\",\n",
    "                \"Disk saturation beyond 95%\",\n",
    "                \"Service mesh circuit breaker open\",\n",
    "            ])\n",
    "        elif level == \"WARN\":\n",
    "            message = random.choice([\n",
    "                \"Retrying connection to cache cluster\",\n",
    "                \"CPU utilization approaching threshold\",\n",
    "                \"Replica lag increasing\",\n",
    "            ])\n",
    "        else:\n",
    "            message = random.choice([\n",
    "                \"Health check passed\",\n",
    "                \"Autoscaler polling\",\n",
    "                \"Background job completed\",\n",
    "            ])\n",
    "        entries.append(f\"{timestamp.isoformat()}Z [{level}] {server_id}: {message}\")\n",
    "    return \"\\n\".join(reversed(entries))\n",
    "\n",
    "\n",
    "def generate_mock_metrics(hours: int = 24) -> pd.DataFrame:\n",
    "    \"\"\"Return hourly CPU/memory stats with spikes to trigger SLA alerts.\"\"\"\n",
    "    now = datetime.utcnow()\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"timestamp\": [now - timedelta(hours=h) for h in range(hours)][::-1],\n",
    "            \"cpu_pct\": [max(10, min(99, random.gauss(55, 18))) for _ in range(hours)],\n",
    "            \"memory_pct\": [max(20, min(95, random.gauss(63, 12))) for _ in range(hours)],\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_incident_email(severity: Literal[\"SEV1\", \"SEV2\", \"SEV3\"]) -> str:\n",
    "    incident = random.choice(INCIDENT_TYPES)\n",
    "    window = random.choice([\"00:00-02:00 UTC\", \"02:00-04:00 UTC\", \"Maintenance window TBD\"])\n",
    "    return (\n",
    "        f\"Subject: {severity} {incident} Incident Update\\n\"\n",
    "        f\"From: it-operations@company.com\\n\"\n",
    "        f\"Body: {incident} team reports anomalies impacting customer latency.\"\n",
    "        f\" Suggested remediation window: {window}.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda4694",
   "metadata": {},
   "source": [
    "## \udcca Simulate Observability Signals\n",
    "\n",
    "We mirror the course pattern of working with synthetic data so you can iterate offline. The helpers below create repeatable logs, metrics, and incident emails to drive the agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6866b0d2",
   "metadata": {},
   "source": [
    "## \udee0ï¸ Wrap data access as ADK tools\n",
    "\n",
    "Following the ADK philosophy, we expose Python helpers as callable tools. The supervisor can then compose them just like in the course samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ac13e",
   "metadata": {},
   "source": [
    "## ðŸ¤– Compose the multi-agent system\n",
    "\n",
    "As in the course notebooks, we define specialist agents and wire them under a supervisor. Each agent inherits the same Gemini model but uses different instructions and toolsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7425ea33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T07:43:47.780487Z",
     "iopub.status.busy": "2025-11-23T07:43:47.779857Z",
     "iopub.status.idle": "2025-11-23T07:44:06.920630Z",
     "shell.execute_reply": "2025-11-23T07:44:06.919667Z",
     "shell.execute_reply.started": "2025-11-23T07:43:47.780301Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# @title Register ADK tools\n",
    "from google.adk.tools import FunctionTool\n",
    "\n",
    "\n",
    "def fetch_logs_tool(server_id: str = \"prod-app-01\") -> str:\n",
    "    \"\"\"Return recent log entries for a server.\"\"\"\n",
    "    return generate_mock_logs(server_id)\n",
    "\n",
    "\n",
    "def summarize_utilization(time_range: str = \"last_24h\") -> dict:\n",
    "    \"\"\"Provide aggregate CPU/Memory stats for the requested window.\"\"\"\n",
    "    df = generate_mock_metrics()\n",
    "    return {\n",
    "        \"time_range\": time_range,\n",
    "        \"average_cpu_pct\": round(df[\"cpu_pct\"].mean(), 2),\n",
    "        \"peak_cpu_pct\": round(df[\"cpu_pct\"].max(), 2),\n",
    "        \"average_memory_pct\": round(df[\"memory_pct\"].mean(), 2),\n",
    "    }\n",
    "\n",
    "\n",
    "def fetch_latest_incident() -> str:\n",
    "    \"\"\"Return the latest synthetic incident email for context.\"\"\"\n",
    "    return generate_incident_email(\"SEV2\")\n",
    "\n",
    "\n",
    "fetch_server_logs = FunctionTool(fetch_logs_tool)\n",
    "get_cpu_utilization = FunctionTool(summarize_utilization)\n",
    "read_incident_emails = FunctionTool(fetch_latest_incident)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5421bd82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T07:44:06.922086Z",
     "iopub.status.busy": "2025-11-23T07:44:06.921575Z",
     "iopub.status.idle": "2025-11-23T07:44:06.932321Z",
     "shell.execute_reply": "2025-11-23T07:44:06.931245Z",
     "shell.execute_reply.started": "2025-11-23T07:44:06.922060Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Multi-agent system ready\n"
     ]
    }
   ],
   "source": [
    "# @title Build supervisor and specialist agents\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.runners import InMemoryRunner\n",
    "\n",
    "log_agent = Agent(\n",
    "    name=\"log_analyst\",\n",
    "    model=MODEL_NAME,\n",
    "    instruction=(\n",
    "        \"You inspect raw infrastructure logs to detect anomalies, downtime, and root causes.\"\n",
    "        \" Summarize key findings and cite log fragments.\"\n",
    "    ),\n",
    "    tools=[fetch_server_logs],\n",
    ")\n",
    "\n",
    "metric_agent = Agent(\n",
    "    name=\"metric_analyst\",\n",
    "    model=MODEL_NAME,\n",
    "    instruction=(\n",
    "        \"You analyze time-series metrics to explain utilization trends, SLA breaches, and capacity risks.\"\n",
    "        \" Produce concise stats and recommendations.\"\n",
    "    ),\n",
    "    tools=[get_cpu_utilization],\n",
    ")\n",
    "\n",
    "operations_agent = Agent(\n",
    "    name=\"operations_planner\",\n",
    "    model=MODEL_NAME,\n",
    "    instruction=(\n",
    "        \"You coordinate remediation windows, patching schedules, and scaling plans using inputs from peers.\"\n",
    "        \" Recommend low-impact execution windows and stakeholder messaging.\"\n",
    "    ),\n",
    "    tools=[get_cpu_utilization, read_incident_emails],\n",
    ")\n",
    "\n",
    "supervisor_agent = Agent(\n",
    "    name=\"it_ops_supervisor\",\n",
    "    model=MODEL_NAME,\n",
    "    instruction=(\n",
    "        \"You orchestrate specialists to answer executive questions about reliability and performance.\"\n",
    "        \" Decide when to call sub-agents and synthesize a single actionable response.\"\n",
    "    ),\n",
    "    sub_agents=[log_agent, metric_agent, operations_agent],\n",
    ")\n",
    "\n",
    "runner = InMemoryRunner(agent=supervisor_agent)\n",
    "print(\"âœ… Multi-agent system ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0510f5dc",
   "metadata": {},
   "source": [
    "## ðŸ§ª Run a simulated incident review\n",
    "\n",
    "This block mirrors the `run_debug` helper showcased in the course notebooks. It streams agent events so you can validate collaboration before wiring dashboards or schedulers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05f1828a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T07:44:06.933632Z",
     "iopub.status.busy": "2025-11-23T07:44:06.933341Z",
     "iopub.status.idle": "2025-11-23T07:44:20.678419Z",
     "shell.execute_reply": "2025-11-23T07:44:20.677437Z",
     "shell.execute_reply.started": "2025-11-23T07:44:06.933602Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User > We had a customer-facing latency spike overnight. Explain root cause, summarize metrics, and propose a mitigation plan.\n",
      "\n",
      "\n",
      " ### Created new session: it-ops-session\n",
      "\n",
      "User > We had a customer-facing latency spike overnight. Explain root cause, summarize metrics, and propose a mitigation plan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it_ops_supervisor > [Calling tool: transfer_to_agent({'agent_name': 'log_analyst'})]\n",
      "it_ops_supervisor > [Tool result: {'result': None}]\n",
      "log_analyst > [Calling tool: fetch_logs_tool({'server_id': 'prod-app-01'})]\n",
      "log_analyst > [Calling tool: fetch_logs_tool({'server_id': 'prod-db-01'})]\n",
      "log_analyst > [Calling tool: fetch_logs_tool({'server_id': 'prod-cache-01'})]\n",
      "log_analyst > [Tool result: {'result': '2025-11-27T01:24:46.444970Z [WARN] prod-app-01: CPU utilization approaching threshold\\n2...]\n",
      "log_analyst > [Tool result: {'result': '2025-11-27T01:24:46.445625Z [WARN] prod-db-01: Retrying connection to cache cluster\\n202...]\n",
      "log_analyst > [Tool result: {'result': '2025-11-27T01:24:46.446218Z [INFO] prod-cache-01: Autoscaler polling\\n2025-11-27T01:29:4...]\n",
      "log_analyst > [Calling tool: fetch_logs_tool({'server_id': 'prod-app-01'})]\n",
      "log_analyst > [Calling tool: fetch_logs_tool({'server_id': 'prod-db-01'})]\n",
      "log_analyst > [Calling tool: fetch_logs_tool({'server_id': 'prod-cache-01'})]\n",
      "log_analyst > [Tool result: {'result': '2025-11-27T01:24:46.444970Z [WARN] prod-app-01: CPU utilization approaching threshold\\n2...]\n",
      "log_analyst > [Tool result: {'result': '2025-11-27T01:24:46.445625Z [WARN] prod-db-01: Retrying connection to cache cluster\\n202...]\n",
      "log_analyst > [Tool result: {'result': '2025-11-27T01:24:46.446218Z [INFO] prod-cache-01: Autoscaler polling\\n2025-11-27T01:29:4...]\n",
      "log_analyst > The latency spike observed overnight appears to be multi-faceted, with contributions from application, database, and cache servers.\n",
      "\n",
      "**Key Findings:**\n",
      "\n",
      "*   **Application Server (prod-app-01):** Experiencing high CPU utilization and disk saturation around 02:49 and 03:34. Database connection timeouts were also noted at 03:14 and 04:44, along with service mesh circuit breaker errors between 03:39 and 03:59. A latency spike was explicitly logged at 05:04.\n",
      "*   **Database Server (prod-db-01):** Showing signs of disk saturation at 03:04 and 03:54, along with repeated database connection timeouts from 01:49 and 04:49. CPU utilization is also high during the period, and replica lag is increasing.\n",
      "*   **Cache Server (prod-cache-01):** Exhibits disk saturation at 02:39 and 02:54, and repeated service mesh circuit breaker errors between 02:34 and 04:19. Database connection timeouts were logged at 02:04, and latency spikes were detected at 02:24 and 04:44.\n",
      "\n",
      "**Root Cause:**\n",
      "\n",
      "The logs suggest a cascading failure:\n",
      "1.  **Initial Stress:** High CPU utilization and increasing replica lag on the application server, coupled with potential issues on the database and cache servers (disk saturation, connection timeouts), likely initiated the problem.\n",
      "2.  **Resource Exhaustion:** Disk saturation on database and cache servers exacerbated the issue, leading to connection timeouts and increased latency.\n",
      "3.  **Service Degradation:** Service mesh circuit breakers opening indicate that the interconnected services were unable to communicate effectively under load, further contributing to latency spikes.\n",
      "4.  **Eventual Latency Spike:** The combination of these factors culminated in the customer-facing latency spike, with specific logs confirming this at 05:04 on the application server.\n",
      "\n",
      "**Metrics Summary:**\n",
      "\n",
      "*   High CPU utilization on `prod-app-01` and `prod-db-01`.\n",
      "*   Significant disk saturation alerts on `prod-db-01` and `prod-cache-01`.\n",
      "*   Increased replica lag on `prod-app-01` and `prod-db-01`.\n",
      "*   Multiple database connection timeouts across all three server types.\n",
      "*   Service mesh circuit breaker errors on `prod-app-01` and `prod-cache-01`.\n",
      "*   Explicit \"Latency spike detected\" messages on `prod-db-01` and `prod-cache-01`, and a final confirmation on `prod-app-01`.\n",
      "\n",
      "**Proposed Mitigation Plan:**\n",
      "\n",
      "1.  **Immediate Actions:**\n",
      "    *   **Scale Resources:** Temporarily increase resources (CPU, memory, disk I/O) for `prod-db-01` and `prod-cache-01` to alleviate disk saturation and connection issues.\n",
      "    *   **Investigate Disk Usage:** Identify and clear unnecessary data or logs consuming disk space on `prod-db-01` and `prod-cache-01`.\n",
      "    *   **Database Performance Tuning:** Analyze database query performance and optimize any slow-running queries contributing to high CPU and replica lag.\n",
      "\n",
      "2.  **Short-Term Actions:**\n",
      "    *   **Review Autoscaling Policies:** Evaluate the effectiveness of current autoscaling configurations for `prod-app-01`, `prod-db-01`, and `prod-cache-01` to ensure they respond adequately to load changes.\n",
      "    *   **Cache Optimization:** Investigate cache hit rates and eviction policies to ensure efficient cache usage and reduce load on the database.\n",
      "    *   **Circuit Breaker Configuration:** Review and tune service mesh circuit breaker settings to prevent premature or overly aggressive tripping during temporary load spikes.\n",
      "\n",
      "3.  **Long-Term Actions:**\n",
      "    *   **Infrastructure Capacity Planning:** Conduct a thorough review of current and projected resource needs, potentially provisioning more robust instance types or implementing sharding for the database.\n",
      "    *   **Monitoring & Alerting Enhancement:** Refine monitoring to capture finer-grained metrics on disk I/O, database connection pool usage, and inter-service communication latency, with more proactive alerting thresholds.\n",
      "    *   **Performance Testing:** Implement regular load and stress testing to simulate high-traffic scenarios and identify potential bottlenecks before they impact production.\n",
      "\n",
      "I recommend transferring to the `operations_planner` agent to further elaborate on the mitigation plan and resource allocation.\n",
      "log_analyst > [Calling tool: transfer_to_agent({'agent_name': 'operations_planner'})]\n",
      "log_analyst > [Tool result: {'result': None}]\n",
      "log_analyst > The latency spike observed overnight appears to be multi-faceted, with contributions from application, database, and cache servers.\n",
      "\n",
      "**Key Findings:**\n",
      "\n",
      "*   **Application Server (prod-app-01):** Experiencing high CPU utilization and disk saturation around 02:49 and 03:34. Database connection timeouts were also noted at 03:14 and 04:44, along with service mesh circuit breaker errors between 03:39 and 03:59. A latency spike was explicitly logged at 05:04.\n",
      "*   **Database Server (prod-db-01):** Showing signs of disk saturation at 03:04 and 03:54, along with repeated database connection timeouts from 01:49 and 04:49. CPU utilization is also high during the period, and replica lag is increasing.\n",
      "*   **Cache Server (prod-cache-01):** Exhibits disk saturation at 02:39 and 02:54, and repeated service mesh circuit breaker errors between 02:34 and 04:19. Database connection timeouts were logged at 02:04, and latency spikes were detected at 02:24 and 04:44.\n",
      "\n",
      "**Root Cause:**\n",
      "\n",
      "The logs suggest a cascading failure:\n",
      "1.  **Initial Stress:** High CPU utilization and increasing replica lag on the application server, coupled with potential issues on the database and cache servers (disk saturation, connection timeouts), likely initiated the problem.\n",
      "2.  **Resource Exhaustion:** Disk saturation on database and cache servers exacerbated the issue, leading to connection timeouts and increased latency.\n",
      "3.  **Service Degradation:** Service mesh circuit breakers opening indicate that the interconnected services were unable to communicate effectively under load, further contributing to latency spikes.\n",
      "4.  **Eventual Latency Spike:** The combination of these factors culminated in the customer-facing latency spike, with specific logs confirming this at 05:04 on the application server.\n",
      "\n",
      "**Metrics Summary:**\n",
      "\n",
      "*   High CPU utilization on `prod-app-01` and `prod-db-01`.\n",
      "*   Significant disk saturation alerts on `prod-db-01` and `prod-cache-01`.\n",
      "*   Increased replica lag on `prod-app-01` and `prod-db-01`.\n",
      "*   Multiple database connection timeouts across all three server types.\n",
      "*   Service mesh circuit breaker errors on `prod-app-01` and `prod-cache-01`.\n",
      "*   Explicit \"Latency spike detected\" messages on `prod-db-01` and `prod-cache-01`, and a final confirmation on `prod-app-01`.\n",
      "\n",
      "**Proposed Mitigation Plan:**\n",
      "\n",
      "1.  **Immediate Actions:**\n",
      "    *   **Scale Resources:** Temporarily increase resources (CPU, memory, disk I/O) for `prod-db-01` and `prod-cache-01` to alleviate disk saturation and connection issues.\n",
      "    *   **Investigate Disk Usage:** Identify and clear unnecessary data or logs consuming disk space on `prod-db-01` and `prod-cache-01`.\n",
      "    *   **Database Performance Tuning:** Analyze database query performance and optimize any slow-running queries contributing to high CPU and replica lag.\n",
      "\n",
      "2.  **Short-Term Actions:**\n",
      "    *   **Review Autoscaling Policies:** Evaluate the effectiveness of current autoscaling configurations for `prod-app-01`, `prod-db-01`, and `prod-cache-01` to ensure they respond adequately to load changes.\n",
      "    *   **Cache Optimization:** Investigate cache hit rates and eviction policies to ensure efficient cache usage and reduce load on the database.\n",
      "    *   **Circuit Breaker Configuration:** Review and tune service mesh circuit breaker settings to prevent premature or overly aggressive tripping during temporary load spikes.\n",
      "\n",
      "3.  **Long-Term Actions:**\n",
      "    *   **Infrastructure Capacity Planning:** Conduct a thorough review of current and projected resource needs, potentially provisioning more robust instance types or implementing sharding for the database.\n",
      "    *   **Monitoring & Alerting Enhancement:** Refine monitoring to capture finer-grained metrics on disk I/O, database connection pool usage, and inter-service communication latency, with more proactive alerting thresholds.\n",
      "    *   **Performance Testing:** Implement regular load and stress testing to simulate high-traffic scenarios and identify potential bottlenecks before they impact production.\n",
      "\n",
      "I recommend transferring to the `operations_planner` agent to further elaborate on the mitigation plan and resource allocation.\n",
      "log_analyst > [Calling tool: transfer_to_agent({'agent_name': 'operations_planner'})]\n",
      "log_analyst > [Tool result: {'result': None}]\n",
      "operations_planner > [Calling tool: summarize_utilization({'time_range': 'last_24h'})]\n",
      "operations_planner > [Tool result: {'time_range': 'last_24h', 'average_cpu_pct': np.float64(54.5), 'peak_cpu_pct': 99.0, 'average_memor...]\n",
      "operations_planner > [Calling tool: summarize_utilization({'time_range': 'last_24h'})]\n",
      "operations_planner > [Tool result: {'time_range': 'last_24h', 'average_cpu_pct': np.float64(54.5), 'peak_cpu_pct': 99.0, 'average_memor...]\n",
      "operations_planner > The latency spike observed overnight was caused by a combination of factors across the application, database, and cache servers.\n",
      "\n",
      "**Root Cause Summary:**\n",
      "\n",
      "The incident appears to be a cascading failure initiated by high CPU utilization and increasing replica lag on the application server, compounded by disk saturation and connection issues on the database and cache servers. This resource exhaustion led to database connection timeouts and service mesh circuit breakers opening, ultimately resulting in the customer-facing latency spike.\n",
      "\n",
      "**Key Metrics:**\n",
      "\n",
      "*   **CPU Utilization:** High CPU utilization was observed on application and database servers, peaking at 99% across the environment in the last 24 hours.\n",
      "*   **Disk Saturation:** Critical levels of disk saturation were reported on the database and cache servers, indicating a lack of free space.\n",
      "*   **Replica Lag:** Increasing replica lag on the application and database servers suggests delays in data replication.\n",
      "*   **Connection Issues:** Numerous database connection timeouts and retries were logged across all server types.\n",
      "*   **Service Mesh Errors:** Service mesh circuit breakers opened, disrupting inter-service communication.\n",
      "*   **Latency Spikes:** Explicit alerts for latency spikes were detected on database, cache, and application servers, confirming the user-visible impact.\n",
      "\n",
      "**Mitigation Plan:**\n",
      "\n",
      "1.  **Immediate Actions:**\n",
      "    *   **Resource Scaling:** Temporarily scale up CPU, memory, and disk I/O for `prod-db-01` and `prod-cache-01`.\n",
      "    *   **Disk Cleanup:** Identify and remove unnecessary data or logs consuming disk space on `prod-db-01` and `prod-cache-01`.\n",
      "    *   **Database Tuning:** Optimize slow database queries.\n",
      "\n",
      "2.  **Short-Term Actions:**\n",
      "    *   **Autoscaling Review:** Evaluate and adjust autoscaling policies to ensure timely resource provisioning during load spikes.\n",
      "    *   **Cache Optimization:** Analyze cache hit rates and tune eviction policies.\n",
      "    *   **Circuit Breaker Tuning:** Adjust service mesh circuit breaker thresholds to be less sensitive to temporary fluctuations.\n",
      "\n",
      "3.  **Long-Term Actions:**\n",
      "    *   **Capacity Planning:** Assess long-term resource needs and consider infrastructure upgrades like more robust instance types or database sharding.\n",
      "    *   **Enhanced Monitoring:** Implement more granular monitoring for disk I/O, connection pools, and inter-service latency with proactive alerting.\n",
      "    *   **Performance Testing:** Conduct regular load and stress testing to preemptively identify bottlenecks.\n",
      "\n",
      "The average CPU utilization over the last 24 hours was 54.5%, with a peak of 99%. Average memory utilization was 62.27%. These metrics indicate that while the system has capacity, specific components were overwhelmed during the incident.\n",
      "operations_planner > The latency spike observed overnight was caused by a combination of factors across the application, database, and cache servers.\n",
      "\n",
      "**Root Cause Summary:**\n",
      "\n",
      "The incident appears to be a cascading failure initiated by high CPU utilization and increasing replica lag on the application server, compounded by disk saturation and connection issues on the database and cache servers. This resource exhaustion led to database connection timeouts and service mesh circuit breakers opening, ultimately resulting in the customer-facing latency spike.\n",
      "\n",
      "**Key Metrics:**\n",
      "\n",
      "*   **CPU Utilization:** High CPU utilization was observed on application and database servers, peaking at 99% across the environment in the last 24 hours.\n",
      "*   **Disk Saturation:** Critical levels of disk saturation were reported on the database and cache servers, indicating a lack of free space.\n",
      "*   **Replica Lag:** Increasing replica lag on the application and database servers suggests delays in data replication.\n",
      "*   **Connection Issues:** Numerous database connection timeouts and retries were logged across all server types.\n",
      "*   **Service Mesh Errors:** Service mesh circuit breakers opened, disrupting inter-service communication.\n",
      "*   **Latency Spikes:** Explicit alerts for latency spikes were detected on database, cache, and application servers, confirming the user-visible impact.\n",
      "\n",
      "**Mitigation Plan:**\n",
      "\n",
      "1.  **Immediate Actions:**\n",
      "    *   **Resource Scaling:** Temporarily scale up CPU, memory, and disk I/O for `prod-db-01` and `prod-cache-01`.\n",
      "    *   **Disk Cleanup:** Identify and remove unnecessary data or logs consuming disk space on `prod-db-01` and `prod-cache-01`.\n",
      "    *   **Database Tuning:** Optimize slow database queries.\n",
      "\n",
      "2.  **Short-Term Actions:**\n",
      "    *   **Autoscaling Review:** Evaluate and adjust autoscaling policies to ensure timely resource provisioning during load spikes.\n",
      "    *   **Cache Optimization:** Analyze cache hit rates and tune eviction policies.\n",
      "    *   **Circuit Breaker Tuning:** Adjust service mesh circuit breaker thresholds to be less sensitive to temporary fluctuations.\n",
      "\n",
      "3.  **Long-Term Actions:**\n",
      "    *   **Capacity Planning:** Assess long-term resource needs and consider infrastructure upgrades like more robust instance types or database sharding.\n",
      "    *   **Enhanced Monitoring:** Implement more granular monitoring for disk I/O, connection pools, and inter-service latency with proactive alerting.\n",
      "    *   **Performance Testing:** Conduct regular load and stress testing to preemptively identify bottlenecks.\n",
      "\n",
      "The average CPU utilization over the last 24 hours was 54.5%, with a peak of 99%. Average memory utilization was 62.27%. These metrics indicate that while the system has capacity, specific components were overwhelmed during the incident.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Event(model_version='gemini-2.5-flash-lite', content=Content(\n",
       "   parts=[\n",
       "     Part(\n",
       "       function_call=FunctionCall(\n",
       "         args={\n",
       "           'agent_name': 'log_analyst'\n",
       "         },\n",
       "         id='adk-2bbe1f9c-0e45-4d28-b356-dc1e37b00efc',\n",
       "         name='transfer_to_agent'\n",
       "       )\n",
       "     ),\n",
       "   ],\n",
       "   role='model'\n",
       " ), grounding_metadata=None, partial=None, turn_complete=None, finish_reason=<FinishReason.STOP: 'STOP'>, error_code=None, error_message=None, interrupted=None, custom_metadata=None, usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "   candidates_token_count=22,\n",
       "   prompt_token_count=322,\n",
       "   prompt_tokens_details=[\n",
       "     ModalityTokenCount(\n",
       "       modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "       token_count=322\n",
       "     ),\n",
       "   ],\n",
       "   total_token_count=344\n",
       " ), live_session_resumption_update=None, input_transcription=None, output_transcription=None, avg_logprobs=None, logprobs_result=None, cache_metadata=None, citation_metadata=None, invocation_id='e-b9af4eeb-904c-460e-9ac0-80a4b01358bc', author='it_ops_supervisor', actions=EventActions(skip_summarization=None, state_delta={}, artifact_delta={}, transfer_to_agent=None, escalate=None, requested_auth_configs={}, requested_tool_confirmations={}, compaction=None, end_of_agent=None, agent_state=None, rewind_before_invocation_id=None), long_running_tool_ids=set(), branch=None, id='7e216d95-be84-410a-897a-e48ecf6944d7', timestamp=1764220785.016373),\n",
       " Event(model_version=None, content=Content(\n",
       "   parts=[\n",
       "     Part(\n",
       "       function_response=FunctionResponse(\n",
       "         id='adk-2bbe1f9c-0e45-4d28-b356-dc1e37b00efc',\n",
       "         name='transfer_to_agent',\n",
       "         response={\n",
       "           'result': None\n",
       "         }\n",
       "       )\n",
       "     ),\n",
       "   ],\n",
       "   role='user'\n",
       " ), grounding_metadata=None, partial=None, turn_complete=None, finish_reason=None, error_code=None, error_message=None, interrupted=None, custom_metadata=None, usage_metadata=None, live_session_resumption_update=None, input_transcription=None, output_transcription=None, avg_logprobs=None, logprobs_result=None, cache_metadata=None, citation_metadata=None, invocation_id='e-b9af4eeb-904c-460e-9ac0-80a4b01358bc', author='it_ops_supervisor', actions=EventActions(skip_summarization=None, state_delta={}, artifact_delta={}, transfer_to_agent='log_analyst', escalate=None, requested_auth_configs={}, requested_tool_confirmations={}, compaction=None, end_of_agent=None, agent_state=None, rewind_before_invocation_id=None), long_running_tool_ids=None, branch=None, id='37f56c51-51b1-47e7-8207-d5ecd87ee477', timestamp=1764220785.646133),\n",
       " Event(model_version='gemini-2.5-flash-lite', content=Content(\n",
       "   parts=[\n",
       "     Part(\n",
       "       function_call=FunctionCall(\n",
       "         args={\n",
       "           'server_id': 'prod-app-01'\n",
       "         },\n",
       "         id='adk-b46c6076-5a8f-4ae5-b8d3-fb178f0336cf',\n",
       "         name='fetch_logs_tool'\n",
       "       )\n",
       "     ),\n",
       "     Part(\n",
       "       function_call=FunctionCall(\n",
       "         args={\n",
       "           'server_id': 'prod-db-01'\n",
       "         },\n",
       "         id='adk-f7b43416-afba-4ee6-9c4b-f3e805fac845',\n",
       "         name='fetch_logs_tool'\n",
       "       )\n",
       "     ),\n",
       "     Part(\n",
       "       function_call=FunctionCall(\n",
       "         args={\n",
       "           'server_id': 'prod-cache-01'\n",
       "         },\n",
       "         id='adk-686b3084-d613-4d83-8fcb-c8181dd773de',\n",
       "         name='fetch_logs_tool'\n",
       "       )\n",
       "     ),\n",
       "   ],\n",
       "   role='model'\n",
       " ), grounding_metadata=None, partial=None, turn_complete=None, finish_reason=<FinishReason.STOP: 'STOP'>, error_code=None, error_message=None, interrupted=None, custom_metadata=None, usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "   candidates_token_count=72,\n",
       "   prompt_token_count=462,\n",
       "   prompt_tokens_details=[\n",
       "     ModalityTokenCount(\n",
       "       modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "       token_count=462\n",
       "     ),\n",
       "   ],\n",
       "   total_token_count=534\n",
       " ), live_session_resumption_update=None, input_transcription=None, output_transcription=None, avg_logprobs=None, logprobs_result=None, cache_metadata=None, citation_metadata=None, invocation_id='e-b9af4eeb-904c-460e-9ac0-80a4b01358bc', author='log_analyst', actions=EventActions(skip_summarization=None, state_delta={}, artifact_delta={}, transfer_to_agent=None, escalate=None, requested_auth_configs={}, requested_tool_confirmations={}, compaction=None, end_of_agent=None, agent_state=None, rewind_before_invocation_id=None), long_running_tool_ids=set(), branch=None, id='558da6b9-7ff0-4980-921a-236377601547', timestamp=1764220785.648406),\n",
       " Event(model_version=None, content=Content(\n",
       "   parts=[\n",
       "     Part(\n",
       "       function_response=FunctionResponse(\n",
       "         id='adk-b46c6076-5a8f-4ae5-b8d3-fb178f0336cf',\n",
       "         name='fetch_logs_tool',\n",
       "         response={\n",
       "           'result': \"\"\"2025-11-27T01:24:46.444970Z [WARN] prod-app-01: CPU utilization approaching threshold\n",
       " 2025-11-27T01:29:46.444970Z [WARN] prod-app-01: Replica lag increasing\n",
       " 2025-11-27T01:34:46.444970Z [WARN] prod-app-01: CPU utilization approaching threshold\n",
       " 2025-11-27T01:39:46.444970Z [INFO] prod-app-01: Autoscaler polling\n",
       " 2025-11-27T01:44:46.444970Z [INFO] prod-app-01: Background job completed\n",
       " 2025-11-27T01:49:46.444970Z [WARN] prod-app-01: Retrying connection to cache cluster\n",
       " 2025-11-27T01:54:46.444970Z [INFO] prod-app-01: Autoscaler polling\n",
       " 2025-11-27T01:59:46.444970Z [WARN] prod-app-01: Retrying connection to cache cluster\n",
       " 2025-11-27T02:04:46.444970Z [INFO] prod-app-01: Health check passed\n",
       " 2025-11-27T02:09:46.444970Z [INFO] prod-app-01: Health check passed\n",
       " 2025-11-27T02:14:46.444970Z [INFO] prod-app-01: Background job completed\n",
       " 2025-11-27T02:19:46.444970Z [WARN] prod-app-01: CPU utilization approaching threshold\n",
       " 2025-11-27T02:24:46.444970Z [INFO] prod-app-01: Health check passed\n",
       " 2025-11-27T02:29:46.444970Z [INFO] prod-app-01: Background job completed\n",
       " 2025-11-27T02:34:46.444970Z [INFO] prod-app-01: Background job completed\n",
       " 2025-11-27T02:39:46.444970Z [INFO] prod-app-01: Background job completed\n",
       " 2025-11-27T02:44:46.444970Z [WARN] prod-app-01: CPU utilization approaching threshold\n",
       " 2025-11-27T02:49:46.444970Z [ERROR] prod-app-01: Disk saturation beyond 95%\n",
       " 2025-11-27T02:54:46.444970Z [INFO] prod-app-01: Autoscaler polling\n",
       " 2025-11-27T02:59:46.444970Z [WARN] prod-app-01: CPU utilization approaching threshold\n",
       " 2025-11-27T03:04:46.444970Z [INFO] prod-app-01: Health check passed\n",
       " 2025-11-27T03:09:46.444970Z [INFO] prod-app-01: Health check passed\n",
       " 2025-11-27T03:14:46.444970Z [ERROR] prod-app-01: Database connection timeout\n",
       " 2025-11-27T03:19:46.444970Z [INFO] prod-app-01: Background job completed\n",
       " 2025-11-27T03:24:46.444970Z [INFO] prod-app-01: Autoscaler polling\n",
       " 2025-11-27T03:29:46.444970Z [INFO] prod-app-01: Background job completed\n",
       " 2025-11-27T03:34:46.444970Z [ERROR] prod-app-01: Disk saturation beyond 95%\n",
       " 2025-11-27T03:39:46.444970Z [ERROR] prod-app-01: Service mesh circuit breaker open\n",
       " 2025-11-27T03:44:46.444970Z [INFO] prod-app-01: Background job completed\n",
       " 2025-11-27T03:49:46.444970Z [INFO] prod-app-01: Health check passed\n",
       " 2025-11-27T03:54:46.444970Z [ERROR] prod-app-01: Disk saturation beyond 95%\n",
       " 2025-11-27T03:59:46.444970Z [ERROR] prod-app-01: Service mesh circuit breaker open\n",
       " 2025-11-27T04:04:46.444970Z [INFO] prod-app-01: Autoscaler polling\n",
       " 2025-11-27T04:09:46.444970Z [WARN] prod-app-01: Retrying connection to cache cluster\n",
       " 2025-11-27T04:14:46.444970Z [INFO] prod-app-01: Autoscaler polling\n",
       " 2025-11-27T04:19:46.444970Z [INFO] prod-app-01: Health check passed\n",
       " 2025-11-27T04:24:46.444970Z [INFO] prod-app-01: Health check passed\n",
       " 2025-11-27T04:29:46.444970Z [WARN] prod-app-01: CPU utilization approaching threshold\n",
       " 2025-11-27T04:34:46.444970Z [INFO] prod-app-01: Health check passed\n",
       " 2025-11-27T04:39:46.444970Z [INFO] prod-app-01: Background job completed\n",
       " 2025-11-27T04:44:46.444970Z [CRITICAL] prod-app-01: Database connection timeout\n",
       " 2025-11-27T04:49:46.444970Z [WARN] prod-app-01: Replica lag increasing\n",
       " 2025-11-27T04:54:46.444970Z [CRITICAL] prod-app-01: Database connection timeout\n",
       " 2025-11-27T04:59:46.444970Z [INFO] prod-app-01: Health check passed\n",
       " 2025-11-27T05:04:46.444970Z [ERROR] prod-app-01: Latency spike detected on API Gateway\n",
       " 2025-11-27T05:09:46.444970Z [WARN] prod-app-01: Replica lag increasing\n",
       " 2025-11-27T05:14:46.444970Z [INFO] prod-app-01: Health check passed\n",
       " 2025-11-27T05:19:46.444970Z [INFO] prod-app-01: Health check passed\"\"\"\n",
       "         }\n",
       "       )\n",
       "     ),\n",
       "     Part(\n",
       "       function_response=FunctionResponse(\n",
       "         id='adk-f7b43416-afba-4ee6-9c4b-f3e805fac845',\n",
       "         name='fetch_logs_tool',\n",
       "         response={\n",
       "           'result': \"\"\"2025-11-27T01:24:46.445625Z [WARN] prod-db-01: Retrying connection to cache cluster\n",
       " 2025-11-27T01:29:46.445625Z [ERROR] prod-db-01: Latency spike detected on API Gateway\n",
       " 2025-11-27T01:34:46.445625Z [INFO] prod-db-01: Health check passed\n",
       " 2025-11-27T01:39:46.445625Z [INFO] prod-db-01: Health check passed\n",
       " 2025-11-27T01:44:46.445625Z [ERROR] prod-db-01: Latency spike detected on API Gateway\n",
       " 2025-11-27T01:49:46.445625Z [ERROR] prod-db-01: Database connection timeout\n",
       " 2025-11-27T01:54:46.445625Z [INFO] prod-db-01: Autoscaler polling\n",
       " 2025-11-27T01:59:46.445625Z [INFO] prod-db-01: Autoscaler polling\n",
       " 2025-11-27T02:04:46.445625Z [WARN] prod-db-01: Replica lag increasing\n",
       " 2025-11-27T02:09:46.445625Z [WARN] prod-db-01: CPU utilization approaching threshold\n",
       " 2025-11-27T02:14:46.445625Z [INFO] prod-db-01: Background job completed\n",
       " 2025-11-27T02:19:46.445625Z [INFO] prod-db-01: Background job completed\n",
       " 2025-11-27T02:24:46.445625Z [ERROR] prod-db-01: Service mesh circuit breaker open\n",
       " 2025-11-27T02:29:46.445625Z [INFO] prod-db-01: Background job completed\n",
       " 2025-11-27T02:34:46.445625Z [INFO] prod-db-01: Health check passed\n",
       " 2025-11-27T02:39:46.445625Z [INFO] prod-db-01: Background job completed\n",
       " 2025-11-27T02:44:46.445625Z [ERROR] prod-db-01: Service mesh circuit breaker open\n",
       " 2025-11-27T02:49:46.445625Z [INFO] prod-db-01: Health check passed\n",
       " 2025-11-27T02:54:46.445625Z [WARN] prod-db-01: Retrying connection to cache cluster\n",
       " 2025-11-27T02:59:46.445625Z [INFO] prod-db-01: Autoscaler polling\n",
       " 2025-11-27T03:04:46.445625Z [ERROR] prod-db-01: Disk saturation beyond 95%\n",
       " 2025-11-27T03:09:46.445625Z [WARN] prod-db-01: Retrying connection to cache cluster\n",
       " 2025-11-27T03:14:46.445625Z [INFO] prod-db-01: Background job completed\n",
       " 2025-11-27T03:19:46.445625Z [INFO] prod-db-01: Background job completed\n",
       " 2025-11-27T03:24:46.445625Z [WARN] prod-db-01: Retrying connection to cache cluster\n",
       " 2025-11-27T03:29:46.445625Z [INFO] prod-db-01: Health check passed\n",
       " 2025-11-27T03:34:46.445625Z [INFO] prod-db-01: Background job completed\n",
       " 2025-11-27T03:39:46.445625Z [INFO] prod-db-01: Autoscaler polling\n",
       " 2025-11-27T03:44:46.445625Z [INFO] prod-db-01: Health check passed\n",
       " 2025-11-27T03:49:46.445625Z [INFO] prod-db-01: Health check passed\n",
       " 2025-11-27T03:54:46.445625Z [CRITICAL] prod-db-01: Disk saturation beyond 95%\n",
       " 2025-11-27T03:59:46.445625Z [WARN] prod-db-01: CPU utilization approaching threshold\n",
       " 2025-11-27T04:04:46.445625Z [WARN] prod-db-01: CPU utilization approaching threshold\n",
       " 2025-11-27T04:09:46.445625Z [INFO] prod-db-01: Background job completed\n",
       " 2025-11-27T04:14:46.445625Z [INFO] prod-db-01: Background job completed\n",
       " 2025-11-27T04:19:46.445625Z [INFO] prod-db-01: Health check passed\n",
       " 2025-11-27T04:24:46.445625Z [INFO] prod-db-01: Health check passed\n",
       " 2025-11-27T04:29:46.445625Z [INFO] prod-db-01: Background job completed\n",
       " 2025-11-27T04:34:46.445625Z [WARN] prod-db-01: CPU utilization approaching threshold\n",
       " 2025-11-27T04:39:46.445625Z [INFO] prod-db-01: Autoscaler polling\n",
       " 2025-11-27T04:44:46.445625Z [INFO] prod-db-01: Autoscaler polling\n",
       " 2025-11-27T04:49:46.445625Z [ERROR] prod-db-01: Database connection timeout\n",
       " 2025-11-27T04:54:46.445625Z [INFO] prod-db-01: Health check passed\n",
       " 2025-11-27T04:59:46.445625Z [INFO] prod-db-01: Health check passed\n",
       " 2025-11-27T05:04:46.445625Z [WARN] prod-db-01: Retrying connection to cache cluster\n",
       " 2025-11-27T05:09:46.445625Z [INFO] prod-db-01: Autoscaler polling\n",
       " 2025-11-27T05:14:46.445625Z [INFO] prod-db-01: Autoscaler polling\n",
       " 2025-11-27T05:19:46.445625Z [ERROR] prod-db-01: Disk saturation beyond 95%\"\"\"\n",
       "         }\n",
       "       )\n",
       "     ),\n",
       "     Part(\n",
       "       function_response=FunctionResponse(\n",
       "         id='adk-686b3084-d613-4d83-8fcb-c8181dd773de',\n",
       "         name='fetch_logs_tool',\n",
       "         response={\n",
       "           'result': \"\"\"2025-11-27T01:24:46.446218Z [INFO] prod-cache-01: Autoscaler polling\n",
       " 2025-11-27T01:29:46.446218Z [WARN] prod-cache-01: Replica lag increasing\n",
       " 2025-11-27T01:34:46.446218Z [WARN] prod-cache-01: CPU utilization approaching threshold\n",
       " 2025-11-27T01:39:46.446218Z [WARN] prod-cache-01: Replica lag increasing\n",
       " 2025-11-27T01:44:46.446218Z [INFO] prod-cache-01: Autoscaler polling\n",
       " 2025-11-27T01:49:46.446218Z [INFO] prod-cache-01: Background job completed\n",
       " 2025-11-27T01:54:46.446218Z [INFO] prod-cache-01: Autoscaler polling\n",
       " 2025-11-27T01:59:46.446218Z [INFO] prod-cache-01: Health check passed\n",
       " 2025-11-27T02:04:46.446218Z [ERROR] prod-cache-01: Database connection timeout\n",
       " 2025-11-27T02:09:46.446218Z [INFO] prod-cache-01: Autoscaler polling\n",
       " 2025-11-27T02:14:46.446218Z [INFO] prod-cache-01: Background job completed\n",
       " 2025-11-27T02:19:46.446218Z [INFO] prod-cache-01: Health check passed\n",
       " 2025-11-27T02:24:46.446218Z [ERROR] prod-cache-01: Latency spike detected on API Gateway\n",
       " 2025-11-27T02:29:46.446218Z [INFO] prod-cache-01: Background job completed\n",
       " 2025-11-27T02:34:46.446218Z [ERROR] prod-cache-01: Service mesh circuit breaker open\n",
       " 2025-11-27T02:39:46.446218Z [ERROR] prod-cache-01: Disk saturation beyond 95%\n",
       " 2025-11-27T02:44:46.446218Z [INFO] prod-cache-01: Background job completed\n",
       " 2025-11-27T02:49:46.446218Z [WARN] prod-cache-01: Retrying connection to cache cluster\n",
       " 2025-11-27T02:54:46.446218Z [ERROR] prod-cache-01: Disk saturation beyond 95%\n",
       " 2025-11-27T02:59:46.446218Z [INFO] prod-cache-01: Autoscaler polling\n",
       " 2025-11-27T03:04:46.446218Z [INFO] prod-cache-01: Background job completed\n",
       " 2025-11-27T03:09:46.446218Z [WARN] prod-cache-01: Replica lag increasing\n",
       " 2025-11-27T03:14:46.446218Z [WARN] prod-cache-01: CPU utilization approaching threshold\n",
       " 2025-11-27T03:19:46.446218Z [WARN] prod-cache-01: CPU utilization approaching threshold\n",
       " 2025-11-27T03:24:46.446218Z [INFO] prod-cache-01: Health check passed\n",
       " 2025-11-27T03:29:46.446218Z [WARN] prod-cache-01: CPU utilization approaching threshold\n",
       " 2025-11-27T03:34:46.446218Z [ERROR] prod-cache-01: Service mesh circuit breaker open\n",
       " 2025-11-27T03:39:46.446218Z [INFO] prod-cache-01: Health check passed\n",
       " 2025-11-27T03:44:46.446218Z [ERROR] prod-cache-01: Latency spike detected on API Gateway\n",
       " 2025-11-27T03:49:46.446218Z [WARN] prod-cache-01: Replica lag increasing\n",
       " 2025-11-27T03:54:46.446218Z [WARN] prod-cache-01: Retrying connection to cache cluster\n",
       " 2025-11-27T03:59:46.446218Z [WARN] prod-cache-01: CPU utilization approaching threshold\n",
       " 2025-11-27T04:04:46.446218Z [INFO] prod-cache-01: Health check passed\n",
       " 2025-11-27T04:09:46.446218Z [WARN] prod-cache-01: Retrying connection to cache cluster\n",
       " 2025-11-27T04:14:46.446218Z [INFO] prod-cache-01: Health check passed\n",
       " 2025-11-27T04:19:46.446218Z [ERROR] prod-cache-01: Service mesh circuit breaker open\n",
       " 2025-11-27T04:24:46.446218Z [INFO] prod-cache-01: Background job completed\n",
       " 2025-11-27T04:29:46.446218Z [INFO] prod-cache-01: Health check passed\n",
       " 2025-11-27T04:34:46.446218Z [WARN] prod-cache-01: CPU utilization approaching threshold\n",
       " 2025-11-27T04:39:46.446218Z [INFO] prod-cache-01: Autoscaler polling\n",
       " 2025-11-27T04:44:46.446218Z [ERROR] prod-cache-01: Latency spike detected on API Gateway\n",
       " 2025-11-27T04:49:46.446218Z [INFO] prod-cache-01: Autoscaler polling\n",
       " 2025-11-27T04:54:46.446218Z [INFO] prod-cache-01: Health check passed\n",
       " 2025-11-27T04:59:46.446218Z [INFO] prod-cache-01: Background job completed\n",
       " 2025-11-27T05:04:46.446218Z [INFO] prod-cache-01: Background job completed\n",
       " 2025-11-27T05:09:46.446218Z [INFO] prod-cache-01: Health check passed\n",
       " 2025-11-27T05:14:46.446218Z [INFO] prod-cache-01: Autoscaler polling\n",
       " 2025-11-27T05:19:46.446218Z [INFO] prod-cache-01: Health check passed\"\"\"\n",
       "         }\n",
       "       )\n",
       "     ),\n",
       "   ],\n",
       "   role='user'\n",
       " ), grounding_metadata=None, partial=None, turn_complete=None, finish_reason=None, error_code=None, error_message=None, interrupted=None, custom_metadata=None, usage_metadata=None, live_session_resumption_update=None, input_transcription=None, output_transcription=None, avg_logprobs=None, logprobs_result=None, cache_metadata=None, citation_metadata=None, invocation_id='e-b9af4eeb-904c-460e-9ac0-80a4b01358bc', author='log_analyst', actions=EventActions(skip_summarization=None, state_delta={}, artifact_delta={}, transfer_to_agent=None, escalate=None, requested_auth_configs={}, requested_tool_confirmations={}, compaction=None, end_of_agent=None, agent_state=None, rewind_before_invocation_id=None), long_running_tool_ids=None, branch=None, id='0b525c73-e760-4f26-9301-e05223860a09', timestamp=1764220786.445384),\n",
       " Event(model_version='gemini-2.5-flash-lite', content=Content(\n",
       "   parts=[\n",
       "     Part(\n",
       "       text=\"\"\"The latency spike observed overnight appears to be multi-faceted, with contributions from application, database, and cache servers.\n",
       " \n",
       " **Key Findings:**\n",
       " \n",
       " *   **Application Server (prod-app-01):** Experiencing high CPU utilization and disk saturation around 02:49 and 03:34. Database connection timeouts were also noted at 03:14 and 04:44, along with service mesh circuit breaker errors between 03:39 and 03:59. A latency spike was explicitly logged at 05:04.\n",
       " *   **Database Server (prod-db-01):** Showing signs of disk saturation at 03:04 and 03:54, along with repeated database connection timeouts from 01:49 and 04:49. CPU utilization is also high during the period, and replica lag is increasing.\n",
       " *   **Cache Server (prod-cache-01):** Exhibits disk saturation at 02:39 and 02:54, and repeated service mesh circuit breaker errors between 02:34 and 04:19. Database connection timeouts were logged at 02:04, and latency spikes were detected at 02:24 and 04:44.\n",
       " \n",
       " **Root Cause:**\n",
       " \n",
       " The logs suggest a cascading failure:\n",
       " 1.  **Initial Stress:** High CPU utilization and increasing replica lag on the application server, coupled with potential issues on the database and cache servers (disk saturation, connection timeouts), likely initiated the problem.\n",
       " 2.  **Resource Exhaustion:** Disk saturation on database and cache servers exacerbated the issue, leading to connection timeouts and increased latency.\n",
       " 3.  **Service Degradation:** Service mesh circuit breakers opening indicate that the interconnected services were unable to communicate effectively under load, further contributing to latency spikes.\n",
       " 4.  **Eventual Latency Spike:** The combination of these factors culminated in the customer-facing latency spike, with specific logs confirming this at 05:04 on the application server.\n",
       " \n",
       " **Metrics Summary:**\n",
       " \n",
       " *   High CPU utilization on `prod-app-01` and `prod-db-01`.\n",
       " *   Significant disk saturation alerts on `prod-db-01` and `prod-cache-01`.\n",
       " *   Increased replica lag on `prod-app-01` and `prod-db-01`.\n",
       " *   Multiple database connection timeouts across all three server types.\n",
       " *   Service mesh circuit breaker errors on `prod-app-01` and `prod-cache-01`.\n",
       " *   Explicit \"Latency spike detected\" messages on `prod-db-01` and `prod-cache-01`, and a final confirmation on `prod-app-01`.\n",
       " \n",
       " **Proposed Mitigation Plan:**\n",
       " \n",
       " 1.  **Immediate Actions:**\n",
       "     *   **Scale Resources:** Temporarily increase resources (CPU, memory, disk I/O) for `prod-db-01` and `prod-cache-01` to alleviate disk saturation and connection issues.\n",
       "     *   **Investigate Disk Usage:** Identify and clear unnecessary data or logs consuming disk space on `prod-db-01` and `prod-cache-01`.\n",
       "     *   **Database Performance Tuning:** Analyze database query performance and optimize any slow-running queries contributing to high CPU and replica lag.\n",
       " \n",
       " 2.  **Short-Term Actions:**\n",
       "     *   **Review Autoscaling Policies:** Evaluate the effectiveness of current autoscaling configurations for `prod-app-01`, `prod-db-01`, and `prod-cache-01` to ensure they respond adequately to load changes.\n",
       "     *   **Cache Optimization:** Investigate cache hit rates and eviction policies to ensure efficient cache usage and reduce load on the database.\n",
       "     *   **Circuit Breaker Configuration:** Review and tune service mesh circuit breaker settings to prevent premature or overly aggressive tripping during temporary load spikes.\n",
       " \n",
       " 3.  **Long-Term Actions:**\n",
       "     *   **Infrastructure Capacity Planning:** Conduct a thorough review of current and projected resource needs, potentially provisioning more robust instance types or implementing sharding for the database.\n",
       "     *   **Monitoring & Alerting Enhancement:** Refine monitoring to capture finer-grained metrics on disk I/O, database connection pool usage, and inter-service communication latency, with more proactive alerting thresholds.\n",
       "     *   **Performance Testing:** Implement regular load and stress testing to simulate high-traffic scenarios and identify potential bottlenecks before they impact production.\n",
       " \n",
       " I recommend transferring to the `operations_planner` agent to further elaborate on the mitigation plan and resource allocation.\"\"\"\n",
       "     ),\n",
       "     Part(\n",
       "       function_call=FunctionCall(\n",
       "         args={\n",
       "           'agent_name': 'operations_planner'\n",
       "         },\n",
       "         id='adk-fdd5e3e2-26c6-47c6-b145-3df3ecfeec20',\n",
       "         name='transfer_to_agent'\n",
       "       )\n",
       "     ),\n",
       "   ],\n",
       "   role='model'\n",
       " ), grounding_metadata=None, partial=None, turn_complete=None, finish_reason=<FinishReason.STOP: 'STOP'>, error_code=None, error_message=None, interrupted=None, custom_metadata=None, usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "   candidates_token_count=1008,\n",
       "   prompt_token_count=6735,\n",
       "   prompt_tokens_details=[\n",
       "     ModalityTokenCount(\n",
       "       modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "       token_count=6735\n",
       "     ),\n",
       "   ],\n",
       "   total_token_count=7743\n",
       " ), live_session_resumption_update=None, input_transcription=None, output_transcription=None, avg_logprobs=None, logprobs_result=None, cache_metadata=None, citation_metadata=None, invocation_id='e-b9af4eeb-904c-460e-9ac0-80a4b01358bc', author='log_analyst', actions=EventActions(skip_summarization=None, state_delta={}, artifact_delta={}, transfer_to_agent=None, escalate=None, requested_auth_configs={}, requested_tool_confirmations={}, compaction=None, end_of_agent=None, agent_state=None, rewind_before_invocation_id=None), long_running_tool_ids=set(), branch=None, id='fb4c8121-b412-4063-a449-2f8e3bfab57a', timestamp=1764220786.448462),\n",
       " Event(model_version=None, content=Content(\n",
       "   parts=[\n",
       "     Part(\n",
       "       function_response=FunctionResponse(\n",
       "         id='adk-fdd5e3e2-26c6-47c6-b145-3df3ecfeec20',\n",
       "         name='transfer_to_agent',\n",
       "         response={\n",
       "           'result': None\n",
       "         }\n",
       "       )\n",
       "     ),\n",
       "   ],\n",
       "   role='user'\n",
       " ), grounding_metadata=None, partial=None, turn_complete=None, finish_reason=None, error_code=None, error_message=None, interrupted=None, custom_metadata=None, usage_metadata=None, live_session_resumption_update=None, input_transcription=None, output_transcription=None, avg_logprobs=None, logprobs_result=None, cache_metadata=None, citation_metadata=None, invocation_id='e-b9af4eeb-904c-460e-9ac0-80a4b01358bc', author='log_analyst', actions=EventActions(skip_summarization=None, state_delta={}, artifact_delta={}, transfer_to_agent='operations_planner', escalate=None, requested_auth_configs={}, requested_tool_confirmations={}, compaction=None, end_of_agent=None, agent_state=None, rewind_before_invocation_id=None), long_running_tool_ids=None, branch=None, id='9c45415b-2b30-4803-8736-8f0fb7d8100e', timestamp=1764220791.067295),\n",
       " Event(model_version='gemini-2.5-flash-lite', content=Content(\n",
       "   parts=[\n",
       "     Part(\n",
       "       function_call=FunctionCall(\n",
       "         args={\n",
       "           'time_range': 'last_24h'\n",
       "         },\n",
       "         id='adk-2832c7a2-1786-41e1-ad44-18c96be45364',\n",
       "         name='summarize_utilization'\n",
       "       )\n",
       "     ),\n",
       "   ],\n",
       "   role='model'\n",
       " ), grounding_metadata=None, partial=None, turn_complete=None, finish_reason=<FinishReason.STOP: 'STOP'>, error_code=None, error_message=None, interrupted=None, custom_metadata=None, usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "   candidates_token_count=23,\n",
       "   prompt_token_count=7875,\n",
       "   prompt_tokens_details=[\n",
       "     ModalityTokenCount(\n",
       "       modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "       token_count=7875\n",
       "     ),\n",
       "   ],\n",
       "   total_token_count=7898\n",
       " ), live_session_resumption_update=None, input_transcription=None, output_transcription=None, avg_logprobs=None, logprobs_result=None, cache_metadata=None, citation_metadata=None, invocation_id='e-b9af4eeb-904c-460e-9ac0-80a4b01358bc', author='operations_planner', actions=EventActions(skip_summarization=None, state_delta={}, artifact_delta={}, transfer_to_agent=None, escalate=None, requested_auth_configs={}, requested_tool_confirmations={}, compaction=None, end_of_agent=None, agent_state=None, rewind_before_invocation_id=None), long_running_tool_ids=set(), branch=None, id='43c09cb4-a382-48fb-9b32-abc7d86503a4', timestamp=1764220791.070594),\n",
       " Event(model_version=None, content=Content(\n",
       "   parts=[\n",
       "     Part(\n",
       "       function_response=FunctionResponse(\n",
       "         id='adk-2832c7a2-1786-41e1-ad44-18c96be45364',\n",
       "         name='summarize_utilization',\n",
       "         response={\n",
       "           'average_cpu_pct': np.float64(54.5),\n",
       "           'average_memory_pct': np.float64(62.27),\n",
       "           'peak_cpu_pct': 99.0,\n",
       "           'time_range': 'last_24h'\n",
       "         }\n",
       "       )\n",
       "     ),\n",
       "   ],\n",
       "   role='user'\n",
       " ), grounding_metadata=None, partial=None, turn_complete=None, finish_reason=None, error_code=None, error_message=None, interrupted=None, custom_metadata=None, usage_metadata=None, live_session_resumption_update=None, input_transcription=None, output_transcription=None, avg_logprobs=None, logprobs_result=None, cache_metadata=None, citation_metadata=None, invocation_id='e-b9af4eeb-904c-460e-9ac0-80a4b01358bc', author='operations_planner', actions=EventActions(skip_summarization=None, state_delta={}, artifact_delta={}, transfer_to_agent=None, escalate=None, requested_auth_configs={}, requested_tool_confirmations={}, compaction=None, end_of_agent=None, agent_state=None, rewind_before_invocation_id=None), long_running_tool_ids=None, branch=None, id='80f70251-e979-4416-81d5-51a235e7f5bb', timestamp=1764220792.219047),\n",
       " Event(model_version='gemini-2.5-flash-lite', content=Content(\n",
       "   parts=[\n",
       "     Part(\n",
       "       text=\"\"\"The latency spike observed overnight was caused by a combination of factors across the application, database, and cache servers.\n",
       " \n",
       " **Root Cause Summary:**\n",
       " \n",
       " The incident appears to be a cascading failure initiated by high CPU utilization and increasing replica lag on the application server, compounded by disk saturation and connection issues on the database and cache servers. This resource exhaustion led to database connection timeouts and service mesh circuit breakers opening, ultimately resulting in the customer-facing latency spike.\n",
       " \n",
       " **Key Metrics:**\n",
       " \n",
       " *   **CPU Utilization:** High CPU utilization was observed on application and database servers, peaking at 99% across the environment in the last 24 hours.\n",
       " *   **Disk Saturation:** Critical levels of disk saturation were reported on the database and cache servers, indicating a lack of free space.\n",
       " *   **Replica Lag:** Increasing replica lag on the application and database servers suggests delays in data replication.\n",
       " *   **Connection Issues:** Numerous database connection timeouts and retries were logged across all server types.\n",
       " *   **Service Mesh Errors:** Service mesh circuit breakers opened, disrupting inter-service communication.\n",
       " *   **Latency Spikes:** Explicit alerts for latency spikes were detected on database, cache, and application servers, confirming the user-visible impact.\n",
       " \n",
       " **Mitigation Plan:**\n",
       " \n",
       " 1.  **Immediate Actions:**\n",
       "     *   **Resource Scaling:** Temporarily scale up CPU, memory, and disk I/O for `prod-db-01` and `prod-cache-01`.\n",
       "     *   **Disk Cleanup:** Identify and remove unnecessary data or logs consuming disk space on `prod-db-01` and `prod-cache-01`.\n",
       "     *   **Database Tuning:** Optimize slow database queries.\n",
       " \n",
       " 2.  **Short-Term Actions:**\n",
       "     *   **Autoscaling Review:** Evaluate and adjust autoscaling policies to ensure timely resource provisioning during load spikes.\n",
       "     *   **Cache Optimization:** Analyze cache hit rates and tune eviction policies.\n",
       "     *   **Circuit Breaker Tuning:** Adjust service mesh circuit breaker thresholds to be less sensitive to temporary fluctuations.\n",
       " \n",
       " 3.  **Long-Term Actions:**\n",
       "     *   **Capacity Planning:** Assess long-term resource needs and consider infrastructure upgrades like more robust instance types or database sharding.\n",
       "     *   **Enhanced Monitoring:** Implement more granular monitoring for disk I/O, connection pools, and inter-service latency with proactive alerting.\n",
       "     *   **Performance Testing:** Conduct regular load and stress testing to preemptively identify bottlenecks.\n",
       " \n",
       " The average CPU utilization over the last 24 hours was 54.5%, with a peak of 99%. Average memory utilization was 62.27%. These metrics indicate that while the system has capacity, specific components were overwhelmed during the incident.\"\"\"\n",
       "     ),\n",
       "   ],\n",
       "   role='model'\n",
       " ), grounding_metadata=None, partial=None, turn_complete=None, finish_reason=<FinishReason.STOP: 'STOP'>, error_code=None, error_message=None, interrupted=None, custom_metadata=None, usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "   candidates_token_count=576,\n",
       "   prompt_token_count=7959,\n",
       "   prompt_tokens_details=[\n",
       "     ModalityTokenCount(\n",
       "       modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "       token_count=7959\n",
       "     ),\n",
       "   ],\n",
       "   total_token_count=8535\n",
       " ), live_session_resumption_update=None, input_transcription=None, output_transcription=None, avg_logprobs=None, logprobs_result=None, cache_metadata=None, citation_metadata=None, invocation_id='e-b9af4eeb-904c-460e-9ac0-80a4b01358bc', author='operations_planner', actions=EventActions(skip_summarization=None, state_delta={}, artifact_delta={}, transfer_to_agent=None, escalate=None, requested_auth_configs={}, requested_tool_confirmations={}, compaction=None, end_of_agent=None, agent_state=None, rewind_before_invocation_id=None), long_running_tool_ids=None, branch=None, id='4581549e-74cb-4da8-ab4b-64d9836000f6', timestamp=1764220792.221471)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Investigate a slowdown\n",
    "query = \"We had a customer-facing latency spike overnight. Explain root cause, summarize metrics, and propose a mitigation plan.\"\n",
    "print(f\"User > {query}\\n\")\n",
    "\n",
    "# run_debug prints a formatted trace similar to Kaggle notebooks.\n",
    "await runner.run_debug(query, user_id=\"exec\", session_id=\"it-ops-session\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c59b74",
   "metadata": {},
   "source": [
    "## ðŸ’¬ Launch a lightweight dashboard chat\n",
    "\n",
    "To stay consistent with course guidance, we prototype a Gradio chat that management stakeholders can use for on-demand insights. You can later promote this to Cloud Run or Vertex AI Agent Builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd3eca9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T07:49:56.953933Z",
     "iopub.status.busy": "2025-11-23T07:49:56.953578Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n",
      "/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/gradio/queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1621, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "  File \"/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/gradio/utils.py\", line 882, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "  File \"/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/gradio/chat_interface.py\", line 553, in __wrapper\n",
      "    return await submit_fn(*args, **kwargs)\n",
      "  File \"/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/gradio/chat_interface.py\", line 941, in _submit_fn\n",
      "    response = await self.fn(*inputs)\n",
      "  File \"/tmp/ipykernel_6182/1508338738.py\", line 13, in respond\n",
      "    async for event in chat_runner.run_async(\n",
      "  File \"/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/google/adk/runners.py\", line 454, in run_async\n",
      "    async for event in agen:\n",
      "  File \"/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/google/adk/runners.py\", line 395, in _run_with_trace\n",
      "    raise ValueError(message)\n",
      "ValueError: Session not found: mgmt-briefing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Start Gradio chat prototype\n",
    "import gradio as gr\n",
    "from google.genai import types\n",
    "\n",
    "chat_runner = InMemoryRunner(agent=supervisor_agent)\n",
    "\n",
    "async def respond(message: str, history: list[tuple[str, str]]):\n",
    "    user_content = types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part.from_text(text=message)],\n",
    "    )\n",
    "    transcript: list[str] = []\n",
    "    async for event in chat_runner.run_async(\n",
    "        user_id=\"dashboard\",\n",
    "        session_id=\"mgmt-briefing\",\n",
    "        new_message=user_content,\n",
    "    ):\n",
    "        if event.author == \"it_ops_supervisor\" and event.content and event.content.parts:\n",
    "            transcript.extend(part.text or \"\" for part in event.content.parts if part.text)\n",
    "    return \"\\n\".join(transcript)\n",
    "\n",
    "iface = gr.ChatInterface(\n",
    "    fn=respond,\n",
    "    title=\"IT Ops Reliability Copilot\",\n",
    "    description=\"Ask about outages, patch windows, or capacity trends.\",\n",
    ")\n",
    "iface.launch(share=False, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93112b44",
   "metadata": {},
   "source": [
    "## ðŸ’» Try the ADK Web Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7b1889",
   "metadata": {},
   "source": [
    "Follow the same pattern as the Day 1 template: scaffold an agent folder, drop in your production multi-agent code, and then point `adk web` at that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5db03dc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T07:45:23.426121Z",
     "iopub.status.busy": "2025-11-23T07:45:23.425877Z",
     "iopub.status.idle": "2025-11-23T07:45:52.436109Z",
     "shell.execute_reply": "2025-11-23T07:45:52.434892Z",
     "shell.execute_reply.started": "2025-11-23T07:45:23.426101Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.12) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "\u001b[32m\n",
      "Agent created in /home/shiftmint/Documents/googleCapstone2/src/it_ops_observability:\n",
      "- .env\n",
      "- __init__.py\n",
      "- agent.py\n",
      "\u001b[0m\n",
      "\u001b[32m\n",
      "Agent created in /home/shiftmint/Documents/googleCapstone2/src/it_ops_observability:\n",
      "- .env\n",
      "- __init__.py\n",
      "- agent.py\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# @title Scaffold ADK agent package (run once per session)\n",
    "!adk create it_ops_observability --model gemini-2.5-flash-lite --api_key $GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1772ee87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T07:48:21.589857Z",
     "iopub.status.busy": "2025-11-23T07:48:21.589496Z",
     "iopub.status.idle": "2025-11-23T07:48:21.601896Z",
     "shell.execute_reply": "2025-11-23T07:48:21.600938Z",
     "shell.execute_reply.started": "2025-11-23T07:48:21.589835Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ADK web agent written to it_ops_observability/agent.py\n"
     ]
    }
   ],
   "source": [
    "# @title Write agent module for Try ADK\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "agent_dir = Path(\"it_ops_observability\")\n",
    "agent_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "agent_source = textwrap.dedent('''\n",
    "    Multi-agent IT observability system for ADK Web.\n",
    "\n",
    "    from __future__ import annotations\n",
    "\n",
    "    import random\n",
    "    from datetime import datetime, timedelta\n",
    "    from typing import Literal\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    from google.adk.agents import Agent\n",
    "    from google.adk.tools import FunctionTool\n",
    "\n",
    "    MODEL_NAME = \"gemini-2.5-flash-lite\"\n",
    "\n",
    "    SEVERITIES = [\"CRITICAL\", \"ERROR\", \"WARN\", \"INFO\"]\n",
    "    INCIDENT_TYPES = [\"Network\", \"Database\", \"Application\", \"Infrastructure\"]\n",
    "\n",
    "\n",
    "    def generate_mock_logs(server_id: str, window_minutes: int = 240) -> str:\n",
    "        \"\"\"Create timestamped log entries with realistic error bursts.\"\"\"\n",
    "        now = datetime.utcnow()\n",
    "        entries: list[str] = []\n",
    "        for minute in range(window_minutes // 5):\n",
    "            timestamp = now - timedelta(minutes=minute * 5)\n",
    "            level = random.choices(SEVERITIES, weights=[0.05, 0.15, 0.3, 0.5])[0]\n",
    "            if level in {\"CRITICAL\", \"ERROR\"}:\n",
    "                message = random.choice(\n",
    "                    [\n",
    "                        \"Latency spike detected on API Gateway\",\n",
    "                        \"Database connection timeout\",\n",
    "                        \"Disk saturation beyond 95%\",\n",
    "                        \"Service mesh circuit breaker open\",\n",
    "                    ]\n",
    "                )\n",
    "            elif level == \"WARN\":\n",
    "                message = random.choice(\n",
    "                    [\n",
    "                        \"Retrying connection to cache cluster\",\n",
    "                        \"CPU utilization approaching threshold\",\n",
    "                        \"Replica lag increasing\",\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                message = random.choice(\n",
    "                    [\n",
    "                        \"Health check passed\",\n",
    "                        \"Autoscaler polling\",\n",
    "                        \"Background job completed\",\n",
    "                    ]\n",
    "                )\n",
    "            entries.append(f\"{timestamp.isoformat()}Z [{level}] {server_id}: {message}\")\n",
    "        return \"\\n\".join(reversed(entries))\n",
    "\n",
    "\n",
    "    def generate_mock_metrics(hours: int = 24) -> pd.DataFrame:\n",
    "        \"\"\"Return hourly CPU/memory stats with spikes to trigger SLA alerts.\"\"\"\n",
    "        now = datetime.utcnow()\n",
    "        cpu = [max(10, min(99, random.gauss(55, 18))) for _ in range(hours)]\n",
    "        memory = [max(20, min(95, random.gauss(63, 12))) for _ in range(hours)]\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"timestamp\": [now - timedelta(hours=h) for h in range(hours)][::-1],\n",
    "                \"cpu_pct\": cpu,\n",
    "                \"memory_pct\": memory,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "    def generate_incident_email(severity: Literal[\"SEV1\", \"SEV2\", \"SEV3\"]) -> str:\n",
    "        incident = random.choice(INCIDENT_TYPES)\n",
    "        window = random.choice([\"00:00-02:00 UTC\", \"02:00-04:00 UTC\", \"Maintenance window TBD\"])\n",
    "        return (\n",
    "            f\"Subject: {severity} {incident} Incident Update\\n\"\n",
    "            f\"From: it-operations@company.com\\n\"\n",
    "            f\"Body: {incident} team reports anomalies impacting customer latency.\"\n",
    "            f\" Suggested remediation window: {window}.\"\n",
    "        )\n",
    "\n",
    "\n",
    "    def fetch_logs_tool(server_id: str = \"prod-app-01\") -> str:\n",
    "        \"\"\"Return recent log entries for a server.\"\"\"\n",
    "        return generate_mock_logs(server_id)\n",
    "\n",
    "\n",
    "    def summarize_utilization(time_range: str = \"last_24h\") -> dict:\n",
    "        \"\"\"Provide aggregate CPU/Memory stats for the requested window.\"\"\"\n",
    "        df = generate_mock_metrics()\n",
    "        return {\n",
    "            \"time_range\": time_range,\n",
    "            \"average_cpu_pct\": round(df[\"cpu_pct\"].mean(), 2),\n",
    "            \"peak_cpu_pct\": round(df[\"cpu_pct\"].max(), 2),\n",
    "            \"average_memory_pct\": round(df[\"memory_pct\"].mean(), 2),\n",
    "        }\n",
    "\n",
    "\n",
    "    def fetch_latest_incident() -> str:\n",
    "        \"\"\"Return the latest synthetic incident email for context.\"\"\"\n",
    "        return generate_incident_email(\"SEV2\")\n",
    "\n",
    "\n",
    "    def create_agent() -> Agent:\n",
    "        \"\"\"Expose the supervisor agent for ADK web.\"\"\"\n",
    "        fetch_server_logs = FunctionTool(fetch_logs_tool)\n",
    "        get_cpu_utilization = FunctionTool(summarize_utilization)\n",
    "        read_incident_emails = FunctionTool(fetch_latest_incident)\n",
    "\n",
    "        log_agent = Agent(\n",
    "            name=\"log_analyst\",\n",
    "            model=MODEL_NAME,\n",
    "            instruction=(\n",
    "                \"You inspect raw infrastructure logs to detect anomalies, downtime, and root causes.\"\n",
    "                \" Summarize key findings and cite log fragments.\"\n",
    "            ),\n",
    "            tools=[fetch_server_logs],\n",
    "        )\n",
    "\n",
    "        metric_agent = Agent(\n",
    "            name=\"metric_analyst\",\n",
    "            model=MODEL_NAME,\n",
    "            instruction=(\n",
    "                \"You analyze time-series metrics to explain utilization trends, SLA breaches, and capacity risks.\"\n",
    "                \" Produce concise stats and recommendations.\"\n",
    "            ),\n",
    "            tools=[get_cpu_utilization],\n",
    "        )\n",
    "\n",
    "        operations_agent = Agent(\n",
    "            name=\"operations_planner\",\n",
    "            model=MODEL_NAME,\n",
    "            instruction=(\n",
    "                \"You coordinate remediation windows, patching schedules, and scaling plans using inputs from peers.\"\n",
    "                \" Recommend low-impact execution windows and stakeholder messaging.\"\n",
    "            ),\n",
    "            tools=[get_cpu_utilization, read_incident_emails],\n",
    "        )\n",
    "\n",
    "        supervisor_agent = Agent(\n",
    "            name=\"it_ops_supervisor\",\n",
    "            model=MODEL_NAME,\n",
    "            instruction=(\n",
    "                \"You orchestrate specialists to answer executive questions about reliability and performance.\"\n",
    "                \" Decide when to call sub-agents and synthesize a single actionable response.\"\n",
    "            ),\n",
    "            sub_agents=[log_agent, metric_agent, operations_agent],\n",
    "        )\n",
    "\n",
    "        return supervisor_agent\n",
    "    '''\n",
    ")\n",
    "\n",
    "agent_path = agent_dir / \"agent.py\"\n",
    "agent_path.write_text(agent_source)\n",
    "\n",
    "init_path = agent_dir / \"__init__.py\"\n",
    "init_path.write_text(\"from .agent import create_agent\\n\\n__all__ = [\\\"create_agent\\\"]\\n\")\n",
    "\n",
    "print(f\"âœ… ADK web agent written to {agent_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b427129",
   "metadata": {},
   "source": [
    "If you plan to launch the UI from Kaggle, you'll need the proxied notebook URL. The helper below mirrors the Day 1 template so you can grab the correct link (skip it when running locally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051cb9de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T07:48:29.024131Z",
     "iopub.status.busy": "2025-11-23T07:48:29.023773Z",
     "iopub.status.idle": "2025-11-23T07:48:29.212999Z",
     "shell.execute_reply": "2025-11-23T07:48:29.212225Z",
     "shell.execute_reply.started": "2025-11-23T07:48:29.024108Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"padding: 15px; border: 2px solid #f0ad4e; border-radius: 8px; background-color: #fef9f0; margin: 20px 0;\">\n",
       "      <div style=\"font-family: sans-serif; margin-bottom: 12px; color: #333; font-size: 1.1em;\"><strong>âš ï¸ When running on Kaggle:</strong></div>\n",
       "      <div style=\"font-family: sans-serif; margin-bottom: 15px; color: #333; line-height: 1.5;\">\n",
       "        Run the web server cell first, leave it running, then click the button below to open the Try ADK UI in a new tab.\n",
       "      </div>\n",
       "      <a href='https://kkb-production.jupyter-proxy.kaggle.net/k/281130715/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2IiwidHlwIjoiSldUIn0..ab2GlKVGputiq4qVacgElA.QKM6hUhdiAqs0LyzyhZ40Yt27wpAkE-2iEHBjf8MLVXB_C37hyKRZvnKuFbJiC0svzej6y3oIPTdqYV5nSfga7PdZk8EdD5v3Ts3zD0jq0eefg3iUAJlptG88jhBdeY6PiYKNK4Pkadn4rqd3KT-6tKwGDUBO78yDYsFbWXzmy7MdQx5icT4A-bdMgF5L4KPYlCSsL0N100V_x7_RW-05ebZVK7vqdx3OhDadkTrIUT4hYpyvAkyr1X8dzxPPChf.nfQNezo23Ii2BwHkTvzAUA/proxy/proxy/8000' target='_blank' style=\"display:inline-block; background-color:#1a73e8; color:white; padding:10px 20px; text-decoration:none; border-radius:25px; font-family:sans-serif; box-shadow:0 2px 5px rgba(0,0,0,0.2);\">\n",
       "        Open Try ADK UI â†—\n",
       "      </a>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Compute optional Kaggle proxy URL\n",
    "import os\n",
    "from IPython.core.display import display, HTML\n",
    "from jupyter_server.serverapp import list_running_servers\n",
    "\n",
    "\n",
    "def get_adk_proxy_url() -> str:\n",
    "    \"\"\"Resolve the proxied URL that Kaggle notebooks expect.\"\"\"\n",
    "    proxy_host = \"https://kkb-production.jupyter-proxy.kaggle.net\"\n",
    "    adk_port = \"8000\"\n",
    "    servers = list(list_running_servers())\n",
    "    if not servers:\n",
    "        raise RuntimeError(\"No running Jupyter servers detected.\")\n",
    "    base_url = servers[0][\"base_url\"]\n",
    "    parts = base_url.split(\"/\")\n",
    "    try:\n",
    "        kernel, token = parts[2], parts[3]\n",
    "    except IndexError as exc:\n",
    "        raise RuntimeError(f\"Could not parse kernel/token from base URL: {base_url}\") from exc\n",
    "    url_prefix = f\"/k/{kernel}/{token}/proxy/proxy/{adk_port}\"\n",
    "    button_html = f\"\"\"\n",
    "    <div style=\"padding: 15px; border: 2px solid #f0ad4e; border-radius: 8px; background-color: #fef9f0; margin: 20px 0;\">\n",
    "      <div style=\\\"font-family: sans-serif; margin-bottom: 12px; color: #333; font-size: 1.1em;\\\"><strong>âš ï¸ When running on Kaggle:</strong></div>\n",
    "      <div style=\\\"font-family: sans-serif; margin-bottom: 15px; color: #333; line-height: 1.5;\\\">\n",
    "        Run the web server cell first, leave it running, then click the button below to open the Try ADK UI in a new tab.\n",
    "      </div>\n",
    "      <a href='{proxy_host}{url_prefix}' target='_blank' style=\\\"display:inline-block; background-color:#1a73e8; color:white; padding:10px 20px; text-decoration:none; border-radius:25px; font-family:sans-serif; box-shadow:0 2px 5px rgba(0,0,0,0.2);\\\">\n",
    "        Open Try ADK UI â†—\n",
    "      </a>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(button_html))\n",
    "    return url_prefix\n",
    "\n",
    "\n",
    "try:\n",
    "    url_prefix\n",
    "except NameError:\n",
    "    url_prefix = \"\"\n",
    "\n",
    "if not url_prefix:\n",
    "    try:\n",
    "        url_prefix = get_adk_proxy_url()\n",
    "    except Exception as exc:\n",
    "        url_prefix = \"\"\n",
    "        print(\"âš ï¸ Running outside Kaggle. Continue to the next cell and omit --url_prefix if you are local.\")\n",
    "        print(f\"Details: {exc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c7ce73",
   "metadata": {},
   "source": [
    "When you are ready to test in the Try ADK UI, run the cell below. It launches the FastAPI web server and binds your multi-agent system so you can interact via the browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdf2d7a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T07:48:35.270537Z",
     "iopub.status.busy": "2025-11-23T07:48:35.269100Z",
     "iopub.status.idle": "2025-11-23T07:49:56.951090Z",
     "shell.execute_reply": "2025-11-23T07:49:56.949587Z",
     "shell.execute_reply.started": "2025-11-23T07:48:35.270507Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.12) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/google/adk/cli/fast_api.py:138: UserWarning: [EXPERIMENTAL] InMemoryCredentialService: This feature is experimental and may change or be removed in future versions without notice. It may introduce breaking changes at any time.\n",
      "  credential_service = InMemoryCredentialService()\n",
      "/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/google/adk/auth/credential_service/in_memory_credential_service.py:33: UserWarning: [EXPERIMENTAL] BaseCredentialService: This feature is experimental and may change or be removed in future versions without notice. It may introduce breaking changes at any time.\n",
      "  super().__init__()\n",
      "/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/google/adk/cli/fast_api.py:138: UserWarning: [EXPERIMENTAL] InMemoryCredentialService: This feature is experimental and may change or be removed in future versions without notice. It may introduce breaking changes at any time.\n",
      "  credential_service = InMemoryCredentialService()\n",
      "/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/google/adk/auth/credential_service/in_memory_credential_service.py:33: UserWarning: [EXPERIMENTAL] BaseCredentialService: This feature is experimental and may change or be removed in future versions without notice. It may introduce breaking changes at any time.\n",
      "  super().__init__()\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m6287\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32m\n",
      "+-----------------------------------------------------------------------------+\n",
      "| ADK Web Server started                                                      |\n",
      "|                                                                             |\n",
      "| For local testing, access at http://127.0.0.1:8000.                         |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m6287\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32m\n",
      "+-----------------------------------------------------------------------------+\n",
      "| ADK Web Server started                                                      |\n",
      "|                                                                             |\n",
      "| For local testing, access at http://127.0.0.1:8000.                         |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:60848 - \"\u001b[1mGET / HTTP/1.1\u001b[0m\" \u001b[33m307 Temporary Redirect\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:60848 - \"\u001b[1mGET /%7Burl_prefix%7D/dev-ui/ HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:60848 - \"\u001b[1mGET / HTTP/1.1\u001b[0m\" \u001b[33m307 Temporary Redirect\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:60848 - \"\u001b[1mGET /%7Burl_prefix%7D/dev-ui/ HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:60862 - \"\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:60862 - \"\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:53476 - \"\u001b[1mGET /%7Burl_prefix%7D/dev-ui/ HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:53476 - \"\u001b[1mGET /%7Burl_prefix%7D/dev-ui/ HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:53484 - \"\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:53484 - \"\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     Shutting down\n",
      "^C\n",
      "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m6287\u001b[0m]\n",
      "\u001b[31mERROR\u001b[0m:    Traceback (most recent call last):\n",
      "  File \"/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/uvicorn/_compat.py\", line 60, in asyncio_run\n",
      "    return loop.run_until_complete(main)\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 636, in run_until_complete\n",
      "    self.run_forever()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/uvicorn/server.py\", line 70, in serve\n",
      "    with self.capture_signals():\n",
      "  File \"/usr/lib/python3.10/contextlib.py\", line 142, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/uvicorn/server.py\", line 331, in capture_signals\n",
      "    signal.raise_signal(captured_signal)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/starlette/routing.py\", line 701, in lifespan\n",
      "    await receive()\n",
      "  File \"/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/uvicorn/lifespan/on.py\", line 137, in receive\n",
      "    return await self.receive_queue.get()\n",
      "  File \"/usr/lib/python3.10/asyncio/queues.py\", line 159, in get\n",
      "    await getter\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "\n",
      "Aborted!\n",
      "\u001b[32mINFO\u001b[0m:     Shutting down\n",
      "^C\n",
      "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m6287\u001b[0m]\n",
      "\u001b[31mERROR\u001b[0m:    Traceback (most recent call last):\n",
      "  File \"/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/uvicorn/_compat.py\", line 60, in asyncio_run\n",
      "    return loop.run_until_complete(main)\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 636, in run_until_complete\n",
      "    self.run_forever()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/uvicorn/server.py\", line 70, in serve\n",
      "    with self.capture_signals():\n",
      "  File \"/usr/lib/python3.10/contextlib.py\", line 142, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/uvicorn/server.py\", line 331, in capture_signals\n",
      "    signal.raise_signal(captured_signal)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/starlette/routing.py\", line 701, in lifespan\n",
      "    await receive()\n",
      "  File \"/home/shiftmint/Documents/googleCapstone2/.venv/lib/python3.10/site-packages/uvicorn/lifespan/on.py\", line 137, in receive\n",
      "    return await self.receive_queue.get()\n",
      "  File \"/usr/lib/python3.10/asyncio/queues.py\", line 159, in get\n",
      "    await getter\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "\n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "# @title Launch ADK web (keeps running)\n",
    "!adk web it_ops_observability --url_prefix {url_prefix}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c2783",
   "metadata": {},
   "source": [
    "While the ADK web command is running, keep the cell active (Interrupt execution to stop it). Treat the proxy URL as sensitiveâ€”never share it publicly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f66f11",
   "metadata": {},
   "source": [
    "## âœ… Next steps toward Level 4\n",
    "\n",
    "- **Add self-evolving behaviors:** introduce an agent that synthesizes new tools based on novel telemetry sources.\n",
    "- **Persist state:** connect ADK session services to Cloud Firestore or AlloyDB for historical analysis.\n",
    "- **Harden deployment:** containerize this notebook logic and publish via Cloud Run or Vertex AI Agent Engine.\n",
    "- **Evaluate performance:** leverage ADK evaluation suites to benchmark agent responses on archived incidents.\n",
    "\n",
    "Keep iterating in alignment with the Kaggle course, and remember to snapshot results for your Dec 1 submission!"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14484960,
     "sourceId": 121144,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
